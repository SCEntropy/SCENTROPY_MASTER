[metadata]
name = meteor-score-evaluator
version = 1.0.0
description = A comprehensive tool for evaluating text generation models using METEOR metric
long_description = file: README.md
long_description_content_type = text/markdown
author = Your Name
author_email = your.email@example.com
license = MIT
license_files = LICENSE
platforms = any
classifiers =
    Development Status :: 4 - Beta
    Intended Audience :: Developers
    Intended Audience :: Science/Research
    License :: OSI Approved :: MIT License
    Operating System :: OS Independent
    Programming Language :: Python :: 3
    Programming Language :: Python :: 3.7
    Programming Language :: Python :: 3.8
    Programming Language :: Python :: 3.9
    Programming Language :: Python :: 3.10
    Programming Language :: Python :: 3.11
    Topic :: Scientific/Engineering :: Artificial Intelligence
    Topic :: Text Processing :: Linguistic

[options]
packages = find:
install_requires =
    numpy>=1.21.0
    tqdm>=4.62.0
python_requires = >=3.7
zip_safe = False

[options.packages.find]
where = src

[bdist_wheel]
universal = 1

[flake8]
max-line-length = 120
extend-ignore = E203, W503
exclude = 
    .git,
    __pycache__,
    build,
    dist,
    *.egg-info,
    .venv

[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short